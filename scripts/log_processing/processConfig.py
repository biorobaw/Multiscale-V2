import glob
import sys
from pythonUtils.BinaryFiles import *
import numpy as np
from scipy import stats
from pythonUtils.VariableLoader import *

""" 
    This file processes the data generated by a configuration of the multiscale experiment.
    In other words, it combines the results of all rats into a single files.
    Results are stored in a data frame in the respective config folder.
    Requirements: 
        Assumes existence of file mazeMetrics.csv in working directory
    Files generated:
        'configData.pickle' : huge data frame with results of all rats
            df = location . episode . rat . steps . normalized
            normalized = steps / minimum steps for location
            (location == -1) -> geometric mean(locations)
        'summary.pickle' : results (steps) after aggregating by rat and calculating statistics
            df = configData.groupby(rat)['steps'].describe()
            df = location . episode . count . mean . std . min . 25% . 50% . 75% . max
        'summaryNormalized.pickle' : idem previous but normalized by min distance 
            df = configData.groupby(rat)['normalized'].describe()
            df = location . episode . count . mean . std . min . 25% . 50% . 75% . max
        'ratGeomMeans.pickle' : steps per episode per rat aggregating by starting location
            df = rat . episode . geom_mean
        'configGeomMeans.pickle' : 
            df = f = episode . steps . avg_geom_mean . std_geom_mean
    
"""


def process_config(base_folder, config):
    base_folder = os.path.join(base_folder, '')
    config_folder = base_folder + config + '/'
    step_size = 0.08

    # find maze of the config
    configs = pd.read_csv(base_folder+'configs.csv', sep='\t')
    configs = configs.drop(columns=['run_id']).drop_duplicates()
    configs = configs.set_index(['config'])
    config_maze = os.path.basename(configs.loc[config]['mazeFile'])

    # find maze metrics
    maze_metrics = pd.read_csv("mazeMetrics.csv")
    metric_gmean = maze_metrics.groupby('maze')['distance'].apply(stats.gmean).reset_index(name='distance')
    metric_gmean['pos'] = -1
    maze_metrics = maze_metrics.append(metric_gmean, ignore_index=True, sort=True)
    maze_metrics = maze_metrics.set_index(['maze', 'pos']).loc[config_maze]
    maze_metrics['minSteps'] = maze_metrics['distance']/step_size

    # get number rats and starting locations in this experiment:
    num_rats = len(glob.glob(config_folder + "r*-V0.bin"))
    num_locations = np.unique(load_int_vector(config_folder+"r0-steps.bin")).size
    num_episodes = read_vector_size(config_folder+"r0-steps.bin")//num_locations

    # create columns of the final data frame:
    rat_ids = np.repeat(np.arange(num_rats), num_locations*num_episodes)
    episode = np.tile(np.repeat(np.arange(num_episodes), num_locations), num_rats)
    locations = np.zeros(num_episodes*num_locations*num_rats, dtype=np.uint32)
    steps = np.zeros(num_episodes*num_locations*num_rats, dtype=np.uint32)

    # load info of each rat
    append_length = num_episodes*num_locations
    for rat_id in range(0, num_rats):
        file_name = config_folder + "r" + str(rat_id) + "-steps.bin"
        file = open(file_name, 'rb')
        rat_starting_locations = load_int_vector(file)
        rat_steps = load_int_vector(file)
        file.close()
        locations[append_length*rat_id: append_length*(rat_id+1)] = rat_starting_locations
        steps[append_length*rat_id: append_length*(rat_id+1)] = rat_steps

    # create data frame from the columns
    config_df = pd.DataFrame({'location': locations, 'episode': episode, 'rat': rat_ids, 'steps': steps})

    # append geometric mean of each episode (assign it 'location = -1')
    episode_gmean = config_df.groupby(['episode', 'rat'])['steps'].apply(stats.gmean).reset_index(name='steps')
    episode_gmean['location'] = -1
    config_df = config_df.append(episode_gmean, ignore_index=True, sort=True)

    # create summary
    summary = config_df.drop(columns='rat').groupby(['location', 'episode']).describe()
    summary.columns = summary.columns.droplevel()
    summary = summary.reset_index()

    # normalize data:
    min_steps_df = maze_metrics.loc[config_df['location']]['minSteps']
    min_steps_df.index = config_df.index
    config_df['normalized'] = config_df['steps'] / min_steps_df
    # print(config_df.columns, config_df.index)

    # combine performance metric of all starting locations using geometric mean:
    rat_geom_means = config_df.groupby(['rat', 'episode'])['normalized']\
        .apply(stats.gmean).reset_index(name='geom_mean')
    config_geom_means = rat_geom_means.groupby(['episode'])['geom_mean'].agg({'geom_mean': ['mean', 'std']})
    config_geom_means.columns = config_geom_means.columns.droplevel()
    config_geom_means = config_geom_means.rename(columns={'mean': 'avg_geom_mean', 'std': 'std_geom_mean'})
    config_geom_means = config_geom_means.reset_index()

    # normalize summary:
    summary_normalized = summary.copy()
    min_steps_df = maze_metrics.loc[summary_normalized['location']]['minSteps']
    min_steps_df.index = summary_normalized.index
    for col in ['mean', 'std', 'min', '25%', '50%', '75%', 'max']:
        summary_normalized[col] = summary_normalized[col] / min_steps_df

    # save pickled data
    config_df.to_pickle(config_folder+'configData.pickle')
    summary.to_pickle(config_folder+"summary.pickle")
    summary_normalized.to_pickle(config_folder+"summaryNormalized.pickle")
    rat_geom_means.to_pickle(config_folder+"ratGeomMeans.pickle")
    config_geom_means.to_pickle(config_folder+"configGeomMeans.pickle")


def process_all_configs(base_folder):
    config_folders = get_list_of_configs(base_folder)
    # config_folders = ['c'+str(i) for i in range(266, 280)]
    for config in config_folders:
        print("processing: ", config)
        process_config(base_folder, config)


if __name__ == '__main__':
    # argv[1] = base folder
    # argv[2] = config
    if len(sys.argv) > 2:
        process_config(sys.argv[1], sys.argv[2])
    else:
        process_all_configs(sys.argv[1])
